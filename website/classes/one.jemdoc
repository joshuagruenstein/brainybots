# jemdoc: menu{MENU}{one.html}, nofooter
== Bayesian Bonanza

You probably know a little bit about probability from reasoning about every day events: 
. The probability of flipping a heads with a fair coin is $\frac{1}{2}$
. The probability that it'll rain tomorrow is $30\%$ 


In this class, this is a good way to think about probability- assigning a number or "likelihood" to a given event, that comes from a space of events we understand. For example, the space of events with one flip of a fair coin is $\{H, T\}$ and to each element in that space, we assign the probability $\frac{1}{2}.$ 


=== Conditional Probability

In the real world, we don't just deal with singular events- we deal with thousands of events, some of which are connected and some of which are not. We're often interested in reasoning about some event's likelihood based on some other event- the usage of partial information that we already have. 

# TODO: maybe work on this block (eg seperate practice problem,
# intuitively explain condition probability)
# Also we should code up a default-hidden block for answers
In probability terms, we like to think about $\mathbb{P}(A|B)$, or the probability that event $A$ occurs given event $B.$ \n
- The likelihood that you're eating ice cream given that it's summer is higher than the likelihood that you're eating ice cream given that it's winter (maybe). 

Intuitively, to figure out what the probability is that $A$ occurs given that $B$ already occurred, we take the set of events where $B$ has occurred, and just within that space, we look for the probability that $A$ also occurred. Try it yourself: 
==== Practice:

Assume we have a fair 6-sided die 
. What's the probability that our roll is odd? 
. What's the probability our roll is a 1?
. What's the probability our roll is a 1 given that we know our roll is odd? 

~~~
{Solution}  
The probability that our roll is odd is $\frac{\text{number of odd outcomes}}{\text{number of outcomes in the space}} = \frac{3}{6} = \frac{1}{2}$

The probability that our roll is 1 is $\frac{\text{number of outcomes with a 1}}{\text{number of outcomes in the space}} = \frac{1}{6}$

Given that we know our roll is odd, we restrict our sample space to the odd outcomes only: 
\(
\{1,3,5\}
\)
Within this space only, we compute the probability that our roll is a 1: 
\(
\frac{\text{number of outcomes with a 1}}{\text{number of outcomes in the space}} = \frac{1}{3}
\)
~~~  

Conditional probability definition: 

\(
\mathbb{P}(A|B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}
\)

~~~
{Notation note}
$A \cap B$, when $A$ and $B$ are sets, indicates the set of all elements belonging to both $A$ and $B.$
~~~


==== Practice: 

1. We toss a fair coin three times and define the following events on the outcome of those three tosses: 

- A is the event that we toss more heads than tails
- B is the event that the first flip is a head

What's the probability that we toss more heads than tails given that the first flip is a head? 

~~~
{Solution}

This question asks for $\mathbb{P}(A|B)$, which we know we can rewrite as
\(
\mathbb{P}(A|B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}
\)

Now, we compute $\mathbb{P}(B)$ and $\mathbb{P}(A\cap B)$. First, we can enumerate all the possible outcomes of the three tosses: 
\(
\{HHH, HHT, HTH, THH, HTT, THT, TTH, TTT\}
\)

Now, the sequences in which the first flip is a head are 
\(
\{HHH, HHT, HTH, HTT\}
\)
so $\mathbb{P}(B) = \frac{1}{2}$. Now, to compute $\mathbb{P}(A\cap B)$ we look for the sequences that both have more heads than tails and begin with a head. The satisfying sequences are 
\(
\{HHH, HHT, HTH\}
\)
so we see that $\mathbb{P}(A\cap B) = \frac{3}{8}$ so 
\(
\begin{aligned}
\mathbb{P}(A|B) &= \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)} \\
&= \frac{\frac{3}{8}}{\frac{1}{2}}\\
&= \frac{3}{4}
\end{aligned}
\)
~~~

2. *(Bonus)* You are standing in front of three doors, and told that a prize lies behind one of the three doors, with equal probability. You pick one of the doors, and the host then opens one of the remaining doors, showing you that there is no prize behind it. Then the host offers you two options:
. Stay with your first pick
. Switch to the last door

What's the probability of winning if you decide to switch instead of staying with your first pick? 

~~~
{Solution}

Under the second strategy, letting event $B$ be the event that the door the host opens does not have the prize, we can compute 

\(
\begin{aligned}
\mathbb{P}(A|B) &= \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)} \\
&= \frac{\frac{4}{9}}{\frac{2}{3}} \\
&= \frac{2}{3}
\end{aligned}
\)

so one should always switch!
~~~

=== Independence

In the probability world, we often care about whether two events are independent or dependent. Note this concept is not the same as causation- we define independence as when the occurrence of one event does not affect the likelihood of the occurrence of a second event. We can define this nicely using conditional probability! 

A is independent of event B when:  

\(
\mathbb{P}(A) = \mathbb{P}(A|B)
\)

==== Practice: 

1. Again, we toss a fair coin three times and define the following events on the outcome of those three tosses: 

- $A$ is the event that we toss more heads than tails
- $B$ is the event that the first flip is a head
- $C$ is the event that the last flip is a head
Is $A$ independent of $B$? Is $B$ independent of $C$? 

~~~
{Solution}

We've already computed that $\mathbb{P}(A|B) = \frac{3}{8}$. Now, looking at all possible outcomes of the three tosses: 
\(
\{HHH, HHT, HTH, THH, HTT, THT, TTH, TTT\}
\)
The sequences in which we toss more heads than tails are 
\(
\{HHH, HHT, HTH, THH\}
\)
so we see that $\mathbb{P}(A) = \frac{1}{2}$, which is not equal to $P(A|B)$, so events $A$ and $B$ are not independent. 

To see if $B$ is independent of $C$, note that above we computed $P(B) = \frac{1}{2}$. Now, 

\(
\begin{aligned}
\mathbb{P}(B|C) &= \frac{\mathbb{P}(B \cap C)}{\mathbb{P}(B)} \\
&= \frac{\frac{2}{8}}{\frac{1}{2}} \\
&= \frac{1}{2}
\end{aligned}
\)

so the events $B$ and $C$ are independent. 
~~~

=== Total Probability Theorem
To set up this result, let's go back to our example with ice cream and the seasons. Let's say we're trying to find the probability you're eating ice cream at any given time during the year. All we know is the following
. Each season lasts for exactly $\frac{1}{4}$ of the year
. When it's summer you're eating ice cream $\frac{3}{4}$ of the time 
. When it's winter you're eating ice cream $\frac{1}{4}$ of the time
. In spring or fall, you're eating ice cream $\frac{1}{2}$ of the time

To get the total probability you're eating ice cream at any point during the year, you'd probably do the following: 
\(
\frac{1}{4}\cdot\frac{3}{4} + \frac{1}{4}\cdot \frac{1}{4} + \frac{1}{4}\cdot\frac{1}{2} + \frac{1}{4}\cdot\frac{1}{2}
\)
Note that this only works because summer, winter, spring, and fall together cover the entire year (and don't overlap), forming what we call a "partition" of the year. The total probability theorem states: 
\(
\begin{aligned}
\mathbb{P}(B) &=  \mathbb{P}(B \cap A_1) + \mathbb{P}(B \cap A_2) + ... + \mathbb{P}(B \cap A_n) \\ 
&= \mathbb{P}(A_1)\mathbb{P}(B|A_1) + \mathbb{P}(A_1)\mathbb{P}(B|A_2) + ... + \mathbb{P}(A_1)\mathbb{P}(B|A_n) \\
&= \sum_{i=1}^n \mathbb{P}(A_i)\mathbb{P}(B|A_i) 
\end{aligned} 
\)
where $A_1, ..., A_n$ forms a partition of the space. Intuitively, we think of this as saying that the probability $B$ occurs is the sum of the likelihoods that $B$ and $A_1$ occur, and $B$ and $A_2$ occur, and so on.
=== Bayes Rule
Bayes' Rule allows us to begin our inference journey! It's a formula that follows from a rearrangement of the conditional probability formula: 
\(
\mathbb{P}(A|B) = \frac{\mathbb{P}(B|A)\mathbb{P}(A)}{\mathbb{P}(B)}
\)

For example, consider the following: 
. let A be the event that a person caught a cold
. let B be the event that a person drank chicken noodle soup

Then, at MIT Medical, we probably know how many people have caught colds: $P(A) = 0.6$, and we happen to know that half of all people drink chicken noodle soup: $P(B) = 0.5$. And we probably know from our clinic data what proportion of sick people drank soup: $P(B|A)= 0.3$. But what we really care about is knowing what proportion of people who drink soup get sick- this would tell us if soup helps! 

Using Bayes Rule, we can compute that 
\(
\begin{aligned}
\mathbb{P}(A|B) &= \frac{\mathbb{P}(B|A)\mathbb{P}(A)}{\mathbb{P}(B)} \\
&= \frac{0.3\cdot 0.6}{0.5}\\
&= 0.36\\
\end{aligned}
\)

so soup helps!
==== Practice: 

Alice is a stupid neural network who does not yet know the difference between muffin and cupcakes. Unfortunately, Alice is allergic to muffins. She has a muffin detector, that is admittedly somewhat faulty. We define the following events: 

- $A$ is the event that a given baked good is a muffin, not a cupcake: $\mathbb{P}(A) = .3$
- $B$ is the event that the detector goes off. We have $\mathbb{P}(B|A) = .9$ and $\mathbb{P}(B|A^c) = .1$. 

Now, if the detector goes off, what's the probability the baked good is a muffin? 

~~~
{Notation note}
$A^c$ denotes the complement of $A$, or the event that $A$ does not occur- in this case it denotes the event that the baked good is a cupcake. 
~~~

~~~
{Solution}

We apply Bayes Rule. We want 

\(
\begin{aligned}
\mathbb{P}(A|B) &= \frac{\mathbb{P}(B|A)\mathbb{P}(A)}{\mathbb{P}(B)} \\
&= \frac{0.9 \cdot 0.3}{\mathbb{P}(B)} \\
\end{aligned}
\)

But how do we compute $\mathbb{P}(B)$? We can apply the total probability theorem! 

\(
\begin{aligned}
\mathbb{P}(B) &= \mathbb{P}(A^c)\mathbb{P}(B|A^c) + \mathbb{P}(A)\mathbb{P}(B|A)\\
&= 0.7\cdot 0.1 + \mathbb 0.3 \cdot 0.9\\
&= 0.34
\end{aligned}
\)

Now, we have 
\(
\begin{aligned}
\mathbb{P}(A|B) &= \frac{0.9 \cdot 0.3}{0.34} \\
& \approx 0.794
\end{aligned}
\)
~~~

=== Random Variables

We've already seen how powerful it is to be able to assign likelihoods to events. It allows us to draw quantitative conclusions about events. However, we can extend this past assigning likelihoods- we use random variables, which we define as a *real-valued function of the outcome of an experiment*. Some examples of random variables: 
. In an experiment in which we toss a coin five times, the number of heads we obtain
. In an experiment in which we roll a six-sided dice once, the number that shows on top
. In an experiment in which we roll a six-sided dice once, two times the number that shows on top
. In an experiment where we grab a person off the street and measure their height, their height in inches

In the second random variable example, the space of values the random variable could take is finite- there are only six possible values. A *discrete* random variable is one in which the space of values is finite (or countably infinite, but don't worry about that). In the fourth random variable example, the space of values the random variable could take on is infinite. This is an example of a *continuous* random variable, where the space of possible values is uncountably infinite. 

Since we're dealing with probabilities, it's useful for us to assign a probability to each possible value of a random variable. First, with discrete random variables, we use what is called a probability mass function. We will denote the probability mass function (PMF) of a random variable $X$ as $p_X(x) = \mathbb{P}(X= x)$. So in the second random variable example, we would have 
\(
\begin{aligned}
p_X(1) &= \frac{1}{6}\\
p_X(2) &= \frac{1}{6} 
\end{aligned}
\)

and in the third random variable example, we would have 
\(
\begin{aligned}
p_X(1) &= 0\\
p_X(2) &= \frac{1}{6} 
\end{aligned}
\)

Note that all pmfs must satisfy $\sum_x p_X(x) = 1$. For continuous random variables, the analog to the probability mass function is the probability density function (pdf). Note that for a continuous random variable $X$, the probability that $X$ takes on a specific value is 0- we instead specify the pdf over intervals. We denote the pdf of continuous random variable $X$ to be $f_X(x)$ such that 
\( 
\mathbb{P}(a \leq X \leq b) = \int_{a}^b f_X(x)
\)
(don't worry too much about this if you don't know calculus). The most common continuous random variable we'll be dealing with in this class is the normal random variable (or Gaussian). 

The Gaussin pdf is specified completely by two parameters, the mean $\mu$ and the standard deviation $\sigma$ of the distribution. The mean is where the distribution is centered, and the standard deviation is a measure of how "spread out" the distribution is. We'll write the Gaussian distribution with mean $\mu$ and standard deviation $\sigma$ as $\mathcal{N}(\mu, \sigma).$ Then, the probability that the distribution takes a specific value $x$ will be written as $\mathcal{N}(x; \mu, \sigma)$.
