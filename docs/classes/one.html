<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="../jemdoc.css" type="text/css" /><title>Bayesian Bonanza</title>
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Information</div>
<div class="menu-item"><a href="../index.html">Home</a></div>
<div class="menu-item"><a href="../docs.html">Robot&nbsp;Docs</a></div>
<div class="menu-item"><a href="https://github.com/joshuagruenstein/brainybots" target="blank">Github</a></div>
<div class="menu-category">Classes</div>
<div class="menu-item"><a href="../classes/one.html" class="current">Bayesian&nbsp;Bonanza</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Bayesian Bonanza</h1>
</div>
<p>You probably know a little bit about probability from reasoning about every day events: </p>
<ol>
<li><p>The probability of flipping a heads with a fair coin is \(\frac{1}{2}\)</p>
</li>
<li><p>The probability that it'll rain tomorrow is \(30\%\) </p>
</li>
</ol>
<p>In this class, this is a good way to think about probability- assigning a number or &ldquo;likelihood&rdquo; to a given event, that comes from a space of events we understand. For example, the space of events with one flip of a fair coin is \(\{H, T\}\) and to each element in that space, we assign the probability \(\frac{1}{2}.\) </p>
<h3>Conditional Probability</h3>
<p>In the real world, we don't just deal with singular events- we deal with thousands of events, some of which are connected and some of which are not. We're often interested in reasoning about some event's likelihood based on some other event- the usage of partial information that we already have. </p>
<p>In probability terms, we like to think about \(\mathbb{P}(A|B)\), or the probability that event \(A\) occurs given event \(B.\) <br /></p>
<ol>
<li><p>The likelihood that you're eating ice cream given that it's summer is higher than the likelihood that you're eating ice cream given that it's winter (maybe). </p>
</li>
<li><p><b>Practice Problem</b>: Assume we have a fair 6-sided die </p>
<ol>
<li><p>What's the probability that our roll is odd? </p>
</li>
<li><p>What's the probability our roll is a 1?</p>
</li>
<li><p>What's the probability our roll is a 1 given that we know our roll is odd? </p>
</li>
</ol>

</li>
</ol>
<p>Conditional probability definition: </p>
<p style="text-align:center">
\[
\mathbb{P}(A|B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}
\]
</p><div class="infoblock">
<div class="blocktitle">Notation note</div>
<div class="blockcontent">
<p>\(A \cap B\), when \(A\) and \(B\) are sets, indicates the set of all elements belonging to both \(A\) and \(B.\)</p>
</div></div>
<h4>Practice: The Monty Hall Problem</h4>
<p>You are standing in front of three doors, and told that a prize lies behind one of the three doors, with equal probability. You pick one of the doors, and the host then opens one of the remaining doors, showing you that there is no prize behind it. Then the host offers you two options:</p>
<ol>
<li><p>Stay with your first pick</p>
</li>
<li><p>Switch to the last door</p>
</li>
</ol>
<p>What's the best strategy?</p>
<div class="infoblock solutionblock">
<div class="blocktitle">Solution<span class="solutionhelp"> (hover to show)</span>:</div>
<div class="blockcontent">
<p>Under the first strategy, our probability of winning (call this event A) is just \(\frac{1}{3}\). But under the second strategy, letting event \(B\) be the event that the door the host opens does not have the prize, we can compute </p>
<p style="text-align:center">
\[
\begin{aligned}
\mathbb{P}(A|B) &amp;= \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)} \\
\end{aligned}
)\

so one should always switch!
&nbsp;&nbsp;&nbsp;

=== Total Probability Theorem
To set up this result, let's go back to our example with ice cream and the seasons. Let's say we're trying to find the probability you're eating ice cream at any given time during the year. All we know is the following
. Each season lasts for exactly \(\frac{1}{4}\) of the year
. When it's summer you're eating ice cream \(\frac{3}{4}\) of the time 
. When it's winter you're eating ice cream \(\frac{1}{4}\) of the time
. In spring or fall, you're eating ice cream \(\frac{1}{2}\) of the time

To get the total probability you're eating ice cream at any point during the year, you'd probably do the following: 
\(
\frac{1}{4}\cdot\frac{3}{4} + \frac{1}{4}\cdot \frac{1}{4} + \frac{1}{4}\cdot\frac{1}{2} + \frac{1}{4}\cdot\frac{1}{2}
\]
</p><p>Note that this only works because summer, winter, spring, and fall together cover the entire year (and don't overlap), forming what we call a &ldquo;partition&rdquo; of the year. The total probability theorem states: </p>
<p style="text-align:center">
\[
\mathbb{P}(B) = \mathbb{P}(A_1)\mathbb{P}(B|A_1) + \mathbb{P}(A_1)\mathbb{P}(B|A_2) + &hellip; + \mathbb{P}(A_1)\mathbb{P}(B|A_n) 
\]
</p><p>where \(A_1, &hellip;, A_n\) forms a partition of the space. </p>
<h3>Bayes Rule</h3>
<p>Bayes&rsquo; Rule allows us to begin our inference journey! It's a formula that follows from a rearrangement of the conditional probability formula: </p>
<p style="text-align:center">
\[
\mathbb{P}(A|B) = \frac{\mathbb{P}(B|A)\mathbb{P}(A)}{\mathbb{P}(B)}
\]
</p><p>For example, consider the following: </p>
<ol>
<li><p>let A be the event that a person caught a cold</p>
</li>
<li><p>let B be the event that a person drank chicken noodle soup</p>
</li>
</ol>
<p>Then, at MIT Medical, we probably know how many people have caught colds: \(P(A) = 0.6\), and we happen to know that half of all people drink chicken noodle soup: \(P(B) = 0.5\). And we probably know from our clinic data what proportion of sick people drank soup: \(P(B|A)= 0.3\). But what we really care about is knowing what proportion of people who drink soup get sick- this would tell us if soup helps! </p>
<p>Using Bayes Rule, we can compute that </p>
<p style="text-align:center">
\[
\begin{aligned}
\mathbb{P}(A|B) &amp;= \frac{\mathbb{P}(B|A)\mathbb{P}(A)}{\mathbb{P}(B)} \\
&amp;= \frac{0.3\cdot 0.6}{0.5}\\
&amp;= 0.36\\
\end{aligned}
\]
</p><p>so soup helps!</p>
</td>
</tr>
</table>
</body>
</html>
