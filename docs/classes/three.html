<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="../jemdoc.css" type="text/css" />
<script src="../index.js"></script>
<title>Markov Madness</title>
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script src="https://cdn.jsdelivr.net/npm/roughjs@4.0.4/bundled/rough.min.js"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Information</div>
<div class="menu-item"><a href="../index.html">Home</a></div>
<div class="menu-item"><a href="../docs.html">Robot&nbsp;Docs</a></div>
<div class="menu-item"><a href="https://github.com/joshuagruenstein/brainybots" target="blank">Github</a></div>
<div class="menu-category">Classes</div>
<div class="menu-item"><a href="../classes/one.html">Bayesian&nbsp;Bonanza</a></div>
<div class="menu-item"><a href="../classes/two.html">Decisions,&nbsp;Decisions</a></div>
<div class="menu-item"><a href="../classes/three.html" class="current">Markov&nbsp;Madness</a></div>
<div class="menu-item"><a href="../classes/four.html">Particle&nbsp;Party</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Markov Madness</h1>
</div>
<p>Imagine Alyssa P. Hacker has two friends: Ben Bitdiddle and Louis Reasoner.  Ben Bitdiddle is also friends with Joe Schmoe, but Joe Schmoe is not friends with either Alyssa or Louis.  One way we could represent this series of friendships is with a simple diagram like below,</p>
<img src="bbimages/friends.png" style="width:300px;height:auto;padding:1em;" class="illustration"/>
<p>where people are represented by circles, or <i>vertices</i>, and the relationships between them are lines, or <i>edges</i>.  In this case we chose to define an edge in our diagram as the presence of a friendship between two friends; however, we could have also constructed a diagram of just the opposite, where an edge means the lack of a friendship:</p>
<img src="bbimages/enemies.png" style="width:300px;height:auto;padding:1em;" class="illustration"/>
<p>These sorts of diagrams, called <i>graphs</i>, are incredibly versatile, and can be used both visually and mathematically to represent relationships between all sorts of things.  More formally, we define a graph \(G = (V,E)\) as </p>
<ul>
<li><p>\(V\): a set of vertices, also called <i>nodes</i>,</p>
</li>
<li><p>and \(E\): a set of edges, which are pairs of vertices.</p>
</li>
</ul>
<p>Note that graphs can contain either directed or undirected edges (but not both). For example, imagine if Alyssa was friends with Ben, but Ben wasn't friends with Alyssa, and all the other friendships remained bidirectional.  We could represent that with the following <i>directed</i> graph:</p>
<img src="bbimages/difriends.png" style="width:300px;height:auto;padding:1em;" class="illustration"/>
<p>Graphs are a natural way in which humans present information: for example, we might represent a system of highways and roads as a directed graph, which an edge going in the direction that traffic flows in.</p>
<p>We're going to try to encode information and their <i>dependencies</i> using directed graphs. Let's consider again the event that you're eating ice cream on a given day. We might say that perhaps this event depends on the season and whether you're hungry. Whether you're hungry might depend on whether you've eaten dinner. A naive way that we might try to encode this might be the graph on the left.</p>
<img src="bbimages/dgex.png" style="width:230px;height:auto; padding:1em" class="illustration"/>
<p class="caption">A directed graphical model about eating ice cream.</p>
<h3>Graphical Models</h3>
<p>This brings us to the idea of graphical models. A graphical model is just a graph that in some way represents a set of distributions over random variables: each vertex represents a random variable (or event) and each edge in some way encodes a dependency. </p>
<p>Graphical models are useful because they give us information about how we can <i>factor</i> the distribution over all the variables (or the <i>joint</i>).  For example, taking a subsection of the above graph, we can compute the probability of having eaten dinner and being hungry as the following:</p>
<p style="text-align:center">
\[
P(\text{dinner}, \text{hunger}) = P(\text{dinner}) \times P(\text{hunger} | \text{dinner}).
\]
</p><p>The intuition behind this is that \(P(\text{dinner})\) is conditionally independent of all other variables; however, the likelyhood of being hungry <i>depends</i> on whether you've eaten dinner, and thus needs to be conditioned on \(\text{dinner}\).  We can extend this example to three variables:</p>
<img src="bbimages/gm1.png" style="width:300px;height:auto;padding:2em" class="illustration"/>
<p>This graph represents the factorization of the joint as </p>
<p style="text-align:center">
\[
p_{x,y,z}(x,y,z) = p_x(x)p_{y|x}(y|x)p_{z|y}(z|y).
\]
</p><p>This also makes some intuitive sense: the graph tells us that \(y\) really only depends on \(x\) and that \(z\) only really depends on \(y\), so a given event \(x,y,z\)'s likelihood can be computed using just those conditional dependencies, and the distribution of \(x\). This graph also yields another important result: we know that the joint can always be factored using the chain rule (from <a href="one.html" target=&ldquo;blank&rdquo;>class one</a>) as </p>
<p style="text-align:center">
\[
p_{x,y,z}(x,y,z) = p_x(x)p_{y|x}(y|x)p_{z|y, x}(z|y, x)
\]
</p><p>So setting those two factorizations equal to each other, we have that </p>
<p style="text-align:center">
\[
p_x(x)p_{y|x}(y|x)p_{z|y, x}(z|y, x) = p_x(x)p_{y|x}(y|x)p_{z|y}(z|y)
\]
</p><p>And simplifying, we find that \(p_{z|y, x}(z|y, x) = p_{z|y}(z|y)\). In other words, two probabilities are the same conditioning on \(x\) and not conditioning on \(x\).  This relationship says that \(z\) and \(x\) are <i>conditionally independent</i> given \(y\).</p>
<p>Let's look at another: </p>
<img src="bbimages/gm2.png" style="width:180px;height:auto;padding-bottom:2em;" class="illustration"/>
<p>This case is called a common cause, because \(x\) and \(z\) both depend on this other variable, \(y\), so they share that common cause. So here, we do something similar with the factorization- we know that given \(y\), \(x\) and \(z\) only depend on \(y\), so our factorization of the joint is: </p>
<p style="text-align:center">
\[
p_{x,y,z}(x,y,z) = p_y(y)p_{x|y}(x|y)p_{z|y}(z|y)
\]
</p><h3>Hidden Markov Models</h3>
<p>Now, armed with the knowledge of these two types of graph structures, we'll look at a very cool model called the <i>Hidden Markov Model</i>. In real life, we often deal with hidden (or <i>latent</i>) variables: things that cause observable events, but that we do not precisely know themselves.</p>
<p>For example, Harry Potter lives in a closet, and only knows what's happening in the outside world when Hermione sends him letters. Harry believes that the real world has two states (rainy and sunny), but Harry can't observe them himself. Hermione also doesn't tell him exactly what the weather is: she just tells him if she wore a sweater today or not.  We can represent this as the following simple graphical model:</p>
<img src="bbimages/hermione.png" style="width:300px;height:auto;padding-bottom:2em;" class="illustration"/>
<p>where Harry can observe Hermione's letters today, but not the weather.  Thus we can call the weather a latent variable.  If we extend this to multiple days, we can see even more dependencies emerge. If it's rainy today, it might be more likely to be rainy tomorrow and so on. But also, if it's rainy today, Hermione is probably more likely to wear a sweater. </p>
<img src="bbimages/hermionehmm.png" style="width:500px;height:auto;padding:2em;padding-top:1em" class="illustration"/>
<p>This dependency structure is called a Hidden Markov Model, and is applicable to a wide range of inference tasks beyond Harry Potter.  In today's lab, we'll use it to localize our robot while exploring a maze!</p>
<img src="bbimages/hmm.png" style="height:auto;padding:1em" class="illustration"/>
<p class="caption">Directed graphical model of an HMM.</p>
<h3>Exercise:</h3>
<p>Derive the factorization of the probability distribution \(p_{x_1,&hellip;,x_N,y_1,&hellip;,y_N}(x_1,&hellip;,x_N,y_1,&hellip;,y_N)\) using the two graphical model structure types we learned above. </p>
<div class="infoblock solutionblock">
<div class="blocktitle">Solution<span class="solutionhelp"> (hover to show)</span>:</div>
<div class="blockcontent">
<p>Since \(x_1\) has no incoming edges (and is therefore dependent on no other nodes), we know there will be a \(p_{x_1}(x_1)\) term. Then, we see that \(y_1\) depends only on \(x_1\), so we multiply by the term \(p_{y_1|x_1}(y_1|x_1)\) to encode the graph that just consists of the nodes \(x_1, y_1\). Now, let's add \(x_2\). We see that it depends only on \(x_1\), so we multiply by the term \(p_{x_2|x_1}(x_2|x_1).\) Continuing in this manner through \(y_n\) and \(x_n\), we get the final factorization </p>
<p style="text-align:center">
\[
p_{x_1,&hellip;,x_N,y_1,&hellip;,y_N}(x_1,&hellip;,x_N,y_1,&hellip;,y_N) = p_{x_1}(x_1)\prod_{i=2}^N p_{x_i|x_{i-1}}(x_i|x_{i-1}) \prod_{j=1}^N p_{y_j|x_j}(y_j|x_j)
\]
</p></div></div>
<div class="infoblock">
<div class="blocktitle">Note for Rigor:</div>
<div class="blockcontent">
<p>The general structure of our HMM development has been</p>
<ol>
<li><p>transferring some idea of latent variable and observations into a graphical model with a particular structure</p>
</li>
<li><p>using that graphical model, and the properties that come from graphical models, to do some cool math and develop an algorithm</p>
</li>
</ol>
<p>The second part of what we've done is rigorous- under the assumptions that the graphical model represents what we've told you it represents, all of the math follows very cleanly from properties of graphical models. The first part is where we've handwaved a little. </p>
<p>It's not immediately obvious that this graph is  correct. In the Harry Potter example, couldn't Tuesday's letter encode a little bit about Monday's weather? Why isn't there an edge between Monday and Tuesday? The reason that we've been able to say that each latent variable depends only on the one prior, and that each observation depends on only one latent variable is due to the definition of Hidden Markov Models. </p>
<p>A Hidden Markov Model is defined to have the Markov Property, also sometimes known as memorylessness. This property encodes that the future, given the present, does not depend at all on the past. Another way to think about that is that the present fully encodes any of the information you may need from the past (this could be all of it, or none of it), and so when thinking about the past, it suffices to think only about the present. In math, we write </p>
<p style="text-align:center">
\[
\mathbb{P}(X_t = x_t | X_{t-1} = x_{t-1}, X_{t-2} = x_{t-2}, &hellip;, X_1 = x_1) = \mathbb{P}(X_t = x_t | X_{t-1} = x_{t-1}
\]
</p><p>So given this mathematical factorization, we could actually derive the factorization of the joint distribution that we did above, and we'd find that it's equal to the factorization derived from the graphical model, demonstrating equivalence of the HMM and the graphical model. If you'd like to do this at home, the way to start is with the chain rule factorization of the joint that we showed in Lecture 2. </p>
</div></div>
<h2>Lab #3: time to explore!</h2>
<p>Last week, we used sensor readings from the robot to identify where we were in the maze.  As you may have noticed, this has some limitations: for example, consider the following robot positions:</p>
<canvas class="illustration" id="maze" style="height:400px"></canvas>
<p class="caption">Two robot positions with identical sensor readings.  How do we decide where we really are?</p>
<script>
make_maze("maze", [
    {pos:3, angle:0, color:'blue'},
    {pos:8, angle:0, color:'green'}
]);
</script>
<p>Even if the green and blue robots were to try every position of their sensor turret, they would still get the same exact readings.  Clearly, in order to determine where they are, the robots must <i>explore</i>!</p>
<p>To implement this, we will consider the robot localization problem as a hidden markov model.  Our hidden states \(X_{0..t}\) will be our robot positions, and \(Y_{0..t}\) will be our sensor readings.  At every timestep we'll take a random movement and then a sensor reading, and use this data to infer our robot position!</p>
<h3>Marginalization</h3>
<p>To derive the forward algorithm, which is the algorithm we'll use to infer our robot position, we'll need one more probability tool, marginalization. First, recall our notation for joint probability distributions, which are probability distribution over multiple variables. We write </p>
<p style="text-align:center">
\[
p_{x,y}(1,1)
\]
</p><p>for the probability that \(X =1\) and \(Y=1\), or \(p_{x,y}(X=1, Y=1)\), but we'll use the shorthand above. </p>
<p>But now, given a joint probability distribution, say for two variables \(X,Y\), we might be interested in the marginal distribution for one variable, e.g. \(p_x(x)\). Let's look at the below joint probability distribution, where \(x\) takes values \(0,1\) and \(y\) takes values \(0,1,2\). </p>
<p style="text-align:center">
\[
\begin{aligned}
p_{x,y}(0,0) &amp;= \frac{4}{36} \\ 
p_{x,y}(0,1) &amp;= \frac{5}{36} \\ 
p_{x,y}(0,2) &amp;= \frac{6}{36} \\ 
p_{x,y}(1,0) &amp;= \frac{6}{36} \\ 
p_{x,y}(1,1) &amp;= \frac{7}{36} \\ 
p_{x,y}(1,2) &amp;= \frac{8}{36} \\ 
\end{aligned}
\]
</p><p>What would you do if I asked you for the probability that \(x=0\)? You'd look at the above joint probability distribution, and take all the probabilities where \(x=0\): </p>
<p style="text-align:center">
\[
\begin{aligned}
p_{x,y}(0,0) &amp;= \frac{4}{36} \\ 
p_{x,y}(0,1) &amp;= \frac{5}{36} \\ 
p_{x,y}(0,2) &amp;= \frac{6}{36} \\ 
\end{aligned}
\]
</p><p>Then you'd probably sum those probabilities, to get the answer </p>
<p style="text-align:center">
\[
\begin{aligned}
p_x(0) &amp;= \frac{4}{36} + \frac{5}{36} + \frac{6}{36} \\
&amp;= \frac{15}{36} \\
\]
</p><p>This is a procedure that works all the time- we can write </p>
<p style="text-align:center">
\[
\mathbb{P}_Y(Y = y_1) = \sum_{i=1}^n \mathbb{P}(x_i, y_1)
\]
</p><p>and we can even do this on bigger joint distributions: </p>
<p style="text-align:center">
\[
\mathbb{P}_Y(X = x_1, Y = y_1) = \sum_{i=1}^n \mathbb{P}(x_1, y_1, z_i)
\]
</p><p>The idea is that to get a variable out of the joint distribution, we sum over all possible values that that variable takes. </p>
<h3>Forward Algorithm </h3>
<p>For our localization problem, the probability that we'd like to find is </p>
<p style="text-align:center">
\[
p(x_t| y_1, &hellip;, y_t) = \frac{p(x_t, y_1, &hellip;, y_t)}{p(y_1, &hellip;, y_t)}
\]
</p><p>but note that it suffices to find \(p(x_t, y_1, &hellip;, y_t)\), since \(p(y_1, &hellip;, y_t)\) can just be thought of as a scaling constant. For simplicity, we will use the notation \(y_{1:t}\) to represent \(y_1,&hellip;,y_t\). Now, we can define </p>
<p style="text-align:center">
\[
\m_t(x_t) = p(x_t, y_{1:t}) 
\]
</p><p>Now, recall that from our graphical model setup of HMM's (or by the Markov property of HMM's) \(x_t\) only depends on the variable \(x_{t-1}\), so if we wanted to get \(p(x_t)\), the only variable we need to marginalize out is \(x_{t-1}\). </p>
<p>So we can write </p>
<p style="text-align:center">
\[
\begin{aligned}
\m_t(x_t) &amp;= p(x_t, y_{1:t}) \\
&amp;= \sum_{x_{t-1}} p(x_t, x_{t-1}, y_{1:t}) \\
&amp;= \sum_{x_{t-1}} p(x_t, x_{t-1}, y_{1:t-1}, y_t) \\
\end{aligned}
\]
</p><p>Now we can use the chain rule to write </p>
<p style="text-align:center">
\[
p(x_t, x_{t-1}, y_{1:t-1}, y_t) = p(y_t|x_t, x_{t-1}, y_{1:t-1})p(x_t|x_{t-1}, y_{1:t-1})p(x_{t-1}, y_{1:t-1})
\]
</p><p>But then, recall that \(y_t\) depends only on \(x_t\), and \(x_t\) depends only on \(x_{t-1}\), so </p>
<p style="text-align:center">
\[
\begin{aligned}
p(y_t|x_t, x_{t-1}, y_{1:t-1}) &amp;= p(y_t|x_t)\\
p(x_t|x_{t-1}, y_{1:t-1}) &amp;= p(x_t|x_{t-1})
\end{aligned}
\]
</p><p>and substituting these into the above, we get </p>
<p style="text-align:center">
\[
m_t(x_t) &amp;= \sum_{x_{t-1}} p(y_t|x_t)p(x_t|x_{t-1})p(x_{t-1}, y_{1:t-1}) \\
&amp;= p(y_t|x_t) \sum_{x_{t-1}} p(x_t|x_{t-1})p(x_{t-1}, y_{1:t-1}) \\
&amp;= p(y_t|x_t) \sum_{x_{t-1}} p(x_t|x_{t-1}) m_{t-1}(x_{t-1}) \\
\]
</p><p>where we can pull \(p(y_t|x_t)\) out of the sum because it does not depend on \(x_{t-1}\). </p>
<h3>Moving the robots</h3>
<p>Up to this point, our robots have been stationary.  We can move our robots using some simple commands: <tt>bot.forward()</tt> to move one square forward, to move one square backward, <tt>bot.left()</tt> to turn 90 degrees to the left, and <tt>bot.right()</tt> to turn 90 degrees to the right.  </p>
<div class="infoblock">
<div class="blocktitle">Safety note</div>
<div class="blockcontent">
<p>After every call to <tt>bot.forward()</tt>, the robot will point its distance sensor forward and check if its safe to move before moving the sensor back to its previous position.  If there is a wall in front of the robot and the robot decides it's unsafe to move, <tt>bot.forward()</tt> will return <tt>False</tt>; otherwise, it will return <tt>True</tt>.  This is to ensure your robot does not run into walls. </p>
</div></div>
<p>Before you move on, test running a few movement commands in the Jupyter notebook.</p>
<h3>Inference building blocks</h3>
<p>Now it's time to begin building our inference pipeline.  For the equation for \(\alpha(x_t)\) in the forward algorithm, we will need \(P(X_0=x_0)\), \(P(X_{t}=x_t|X_{t-1}=x_{t-1}, U_{t}=u_{t})\), and \(P(Y_t=y_t | X_t=x_t)\).  Derive each of these expressions and implement them as Python methods in the Jupyter notebook.  Assume that the robot has equal chance of starting in any state.</p>
<div class="infoblock">
<div class="blocktitle">Implementation hint</div>
<div class="blockcontent">
<p>The method <tt>s_new = gm.simulatedAction(s, u)</tt> returns the state the robot would be in after performing action <tt>u</tt> in state <tt>s</tt>.  <tt>u</tt> must be one of strings <tt>'LEFT&rsquo;</tt>, <tt>'RIGHT&rsquo;</tt>, or <tt>'FORWARD&rsquo;</tt>, and <tt>s</tt> must be a tuple of a position integer (0 through 8) and an angle integer in degrees, going clockwise from 0 as north.  This will return a new state in the same format as <tt>s</tt>.</p>
</div></div>
<div class="infoblock solutionblock">
<div class="blocktitle">Solution<span class="solutionhelp"> (hover to show)</span>:</div>
<div class="blockcontent">
<p>We are given that each initial state is equally likely, and we have \(9 \times 4=36\) possible states.  Thus,</p>
<p style="text-align:center">
\[
    P(X_0=x_0) = \frac{1}{36} \approx 0.0277
\]
</p><p>We also seek \(P(X_{t}=x_t|X_{t-1}=x_{t-1}, U_{t}=u_{t})\), or the probability of transitioning to a state \(x_t\) from state \(x_{t-1}\) given action \(u_t\).  This will be either \(1\) or \(0\), as our actions are deterministic (the same thing happens every time when action \(u\) is taken from state \(t\)).  In code, this can be implemented as <tt>int(x_t==gm.simulatedAction(x_tminusone, u))</tt>.</p>
<p>Finally, we can implement \(P(Y_t=y_t | X_t=x_t)\) as a gaussian with mean \(\mu_t\) set by where we are in the maze (\(x_t\)).  So, an implementation using only a forwards sensor reading could be \(\mathcal N(y_t, \mu=\mu_t, \sigma=5)\) with <tt>mu_t = gm.simulatedDistance(x_t[0], x_t[1])</tt> using the previous convention of state being a tuple of <tt>(position, angle)</tt>.</p>
</div></div>
<h3>Putting it together</h3>
<p>Implement a random walk on your robot in a loop by choosing randomly at each iteration whether to go forwards, turn left, or turn right.  Then, implement the forward algorithm and print the most likely robot state at each iteration!</p>
<p>Once you've implemented the system, try it out from a few starting places in the maze.  How many iterations does it take to converge to the right position?  Does your implementation always converge?  Discuss some strengths and limitations of this system with your group.</p>
<div class="infoblock">
<div class="blocktitle">Challenge problem</div>
<div class="blockcontent">
<p>If your initial solution only used one sensor reading per iteration, try upgrading to using sensor readings from multiple angles.  Can you make other improvements that allow your robot to localize quicker?</p>
</div></div>
</td>
</tr>
</table>
</body>
</html>
